{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "tf.random.set_seed(777)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training entries: 25000, labels: 25000\n",
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "imdb = keras.datasets.imdb\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
    "print(\"Training entries: {}, labels: {}\".format(len(train_data), len(train_labels)))\n",
    "print(train_data[0]) \n",
    "#학습에 쓰이는 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMDB DATA를 Vector를 실재 값으로 변환하여 출력 / 정수를 단어로 다시 변환하기\n",
    "\n",
    "#단어와 정수 인덱스를 매핑한 딕셔너리\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "#처음 몇개의 인덱스는 사전에 정의\n",
    "word_index = {k:(v+3) for k,v in word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2 #unknown\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_review(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"리뷰-정수 배열-는 신경망에 주입하기 전에 텐서로 변환되어야한다. 변환하는 방법에는,\n",
    "원-핫 인코딩(one-hot encoding)은 정수 배열을 0과 1로 이루어진 벡터로 변환 \n",
    "예를들어, 배열[3,5]을 인덱스 3과 5만 1이고 나머지는 모두 0인 10,000차원 벡터로 변환 할 수 있다. \n",
    "그다음 실수 벡터 데이터를 다룰 수 있는 층-Dense 층-을 신경망의 첫번째 층으로 사용한다.\n",
    "이 방법은 num_words * num_reviews 크기의 행렬이 필요하기 때문에 메모리를 많이 사용한다.\n",
    "\n",
    "다른 방법으로는, 정수 배열의 길이가 모두 같도록 패딩(padding)을 추가해 max_length * num_reviews\n",
    "크기의 정수 텐서를 만든다.\n",
    "이런 형태의 텐서를 다룰 수 있는 임베딩 층을 신경망의 첫 번째 층으로 사용할 수 있다.\"\"\"\n",
    "\n",
    "#pad_sequences 함수를 이용해서 길이를 맞춘다.\n",
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data,\n",
    "                                                       value=word_index[\"<PAD>\"],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=256)\n",
    "\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(test_data,\n",
    "                                                      value=word_index[\"<PAD>\"],\n",
    "                                                      padding='post',\n",
    "                                                      maxlen=256)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1   14   22   16   43  530  973 1622 1385   65  458 4468   66 3941\n",
      "    4  173   36  256    5   25  100   43  838  112   50  670    2    9\n",
      "   35  480  284    5  150    4  172  112  167    2  336  385   39    4\n",
      "  172 4536 1111   17  546   38   13  447    4  192   50   16    6  147\n",
      " 2025   19   14   22    4 1920 4613  469    4   22   71   87   12   16\n",
      "   43  530   38   76   15   13 1247    4   22   17  515   17   12   16\n",
      "  626   18    2    5   62  386   12    8  316    8  106    5    4 2223\n",
      " 5244   16  480   66 3785   33    4  130   12   16   38  619    5   25\n",
      "  124   51   36  135   48   25 1415   33    6   22   12  215   28   77\n",
      "   52    5   14  407   16   82    2    8    4  107  117 5952   15  256\n",
      "    4    2    7 3766    5  723   36   71   43  530  476   26  400  317\n",
      "   46    7    4    2 1029   13  104   88    4  381   15  297   98   32\n",
      " 2071   56   26  141    6  194 7486   18    4  226   22   21  134  476\n",
      "   26  480    5  144   30 5535   18   51   36   28  224   92   25  104\n",
      "    4  226   65   16   38 1334   88   12   16  283    5   16 4472  113\n",
      "  103   32   15   16 5345   19  178   32    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_5 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,289\n",
      "Trainable params: 160,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, 16, input_shape=(None,)))\n",
    "\"\"\"Embedding 층 정수로 인코딩된 단어를 입력 받고, 각 단어 인덱스에 해당하는 임베딩 벡터를 찾는다.\n",
    "모델이 훈련되면서 학습된다. 최종 차원(batch, sequence, embedding\"\"\"\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "\"\"\"sequence 차원에 대하여 평균을 계산하여 각 샘플에 대해 고정된 길이의 출력 벡터를 반환\n",
    "이는 길이가 다른 입력을 다루는 가장 간단한 방법\"\"\"\n",
    "model.add(keras.layers.Dense(16, activation='relu'))\n",
    "\"\"\"고정 길이의 출력 벡터는 16개의 은닉유닛을 가진 완전연결층을 거친다.\"\"\"\n",
    "model.add(keras.layers.Dense(1, activation ='sigmoid'))\n",
    "\"\"\"마지막 층은 하나의 출력노드를 가진 완전 연결층 sigmiod 활성화 함수를 사용하여, 0과 1 사이의 실수를 출력하여, 이값은 확률 또는 신뢰도\"\"\"\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = train_data[:10000]\n",
    "partial_x_train = train_data[10000:]\n",
    "\n",
    "y_val = train_labels[:10000]\n",
    "partial_y_train = train_labels[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      "15000/15000 [==============================] - 0s 17us/sample - loss: 0.0880 - accuracy: 0.9777 - val_loss: 0.3147 - val_accuracy: 0.8823\n",
      "Epoch 2/40\n",
      "15000/15000 [==============================] - 0s 14us/sample - loss: 0.0844 - accuracy: 0.9790 - val_loss: 0.3177 - val_accuracy: 0.8818\n",
      "Epoch 3/40\n",
      "15000/15000 [==============================] - 0s 13us/sample - loss: 0.0814 - accuracy: 0.9809 - val_loss: 0.3225 - val_accuracy: 0.8807\n",
      "Epoch 4/40\n",
      "15000/15000 [==============================] - 0s 14us/sample - loss: 0.0782 - accuracy: 0.9817 - val_loss: 0.3257 - val_accuracy: 0.8807\n",
      "Epoch 5/40\n",
      "15000/15000 [==============================] - 0s 14us/sample - loss: 0.0751 - accuracy: 0.9827 - val_loss: 0.3296 - val_accuracy: 0.8806\n",
      "Epoch 6/40\n",
      "15000/15000 [==============================] - 0s 14us/sample - loss: 0.0720 - accuracy: 0.9833 - val_loss: 0.3346 - val_accuracy: 0.8800\n",
      "Epoch 7/40\n",
      "15000/15000 [==============================] - 0s 14us/sample - loss: 0.0694 - accuracy: 0.9843 - val_loss: 0.3389 - val_accuracy: 0.8789\n",
      "Epoch 8/40\n",
      "15000/15000 [==============================] - 0s 14us/sample - loss: 0.0668 - accuracy: 0.9856 - val_loss: 0.3421 - val_accuracy: 0.8790\n",
      "Epoch 9/40\n",
      "15000/15000 [==============================] - 0s 14us/sample - loss: 0.0638 - accuracy: 0.9865 - val_loss: 0.3474 - val_accuracy: 0.8787\n",
      "Epoch 10/40\n",
      "15000/15000 [==============================] - 0s 14us/sample - loss: 0.0616 - accuracy: 0.9867 - val_loss: 0.3511 - val_accuracy: 0.8794\n",
      "Epoch 11/40\n",
      "15000/15000 [==============================] - 0s 13us/sample - loss: 0.0589 - accuracy: 0.9879 - val_loss: 0.3569 - val_accuracy: 0.8771\n",
      "Epoch 12/40\n",
      "15000/15000 [==============================] - 0s 14us/sample - loss: 0.0570 - accuracy: 0.9881 - val_loss: 0.3607 - val_accuracy: 0.8785\n",
      "Epoch 13/40\n",
      "15000/15000 [==============================] - 0s 13us/sample - loss: 0.0544 - accuracy: 0.9896 - val_loss: 0.3656 - val_accuracy: 0.8784\n",
      "Epoch 14/40\n",
      "15000/15000 [==============================] - 0s 14us/sample - loss: 0.0521 - accuracy: 0.9901 - val_loss: 0.3704 - val_accuracy: 0.8778\n",
      "Epoch 15/40\n",
      "15000/15000 [==============================] - 0s 14us/sample - loss: 0.0503 - accuracy: 0.9909 - val_loss: 0.3758 - val_accuracy: 0.8762\n",
      "Epoch 16/40\n",
      "15000/15000 [==============================] - 0s 14us/sample - loss: 0.0481 - accuracy: 0.9913 - val_loss: 0.3811 - val_accuracy: 0.8760\n",
      "Epoch 17/40\n",
      "15000/15000 [==============================] - 0s 14us/sample - loss: 0.0464 - accuracy: 0.9919 - val_loss: 0.3895 - val_accuracy: 0.8716\n",
      "Epoch 18/40\n",
      "15000/15000 [==============================] - 0s 13us/sample - loss: 0.0450 - accuracy: 0.9915 - val_loss: 0.3907 - val_accuracy: 0.8763\n",
      "Epoch 19/40\n",
      "15000/15000 [==============================] - 0s 14us/sample - loss: 0.0424 - accuracy: 0.9927 - val_loss: 0.3964 - val_accuracy: 0.8756\n",
      "Epoch 20/40\n",
      "15000/15000 [==============================] - 0s 14us/sample - loss: 0.0408 - accuracy: 0.9935 - val_loss: 0.4009 - val_accuracy: 0.8755\n",
      "Epoch 21/40\n",
      "15000/15000 [==============================] - 0s 14us/sample - loss: 0.0391 - accuracy: 0.9937 - val_loss: 0.4067 - val_accuracy: 0.8745\n",
      "Epoch 22/40\n",
      "15000/15000 [==============================] - 0s 14us/sample - loss: 0.0376 - accuracy: 0.9944 - val_loss: 0.4129 - val_accuracy: 0.8734\n",
      "Epoch 23/40\n",
      "15000/15000 [==============================] - 0s 13us/sample - loss: 0.0362 - accuracy: 0.9943 - val_loss: 0.4172 - val_accuracy: 0.8736\n",
      "Epoch 24/40\n",
      "15000/15000 [==============================] - 0s 14us/sample - loss: 0.0349 - accuracy: 0.9951 - val_loss: 0.4238 - val_accuracy: 0.8712\n",
      "Epoch 25/40\n",
      "15000/15000 [==============================] - 0s 14us/sample - loss: 0.0338 - accuracy: 0.9953 - val_loss: 0.4282 - val_accuracy: 0.8719\n",
      "Epoch 26/40\n",
      "15000/15000 [==============================] - 0s 15us/sample - loss: 0.0319 - accuracy: 0.9953 - val_loss: 0.4344 - val_accuracy: 0.8704\n",
      "Epoch 27/40\n",
      "15000/15000 [==============================] - 0s 14us/sample - loss: 0.0308 - accuracy: 0.9961 - val_loss: 0.4394 - val_accuracy: 0.8717\n",
      "Epoch 28/40\n",
      "15000/15000 [==============================] - 0s 14us/sample - loss: 0.0296 - accuracy: 0.9962 - val_loss: 0.4449 - val_accuracy: 0.8711\n",
      "Epoch 29/40\n",
      "15000/15000 [==============================] - 0s 15us/sample - loss: 0.0282 - accuracy: 0.9965 - val_loss: 0.4504 - val_accuracy: 0.8699\n",
      "Epoch 30/40\n",
      "15000/15000 [==============================] - 0s 14us/sample - loss: 0.0272 - accuracy: 0.9962 - val_loss: 0.4562 - val_accuracy: 0.8696\n",
      "Epoch 31/40\n",
      "15000/15000 [==============================] - 0s 14us/sample - loss: 0.0261 - accuracy: 0.9971 - val_loss: 0.4632 - val_accuracy: 0.8701\n",
      "Epoch 32/40\n",
      "15000/15000 [==============================] - 0s 13us/sample - loss: 0.0251 - accuracy: 0.9969 - val_loss: 0.4678 - val_accuracy: 0.8695\n",
      "Epoch 33/40\n",
      "15000/15000 [==============================] - 0s 13us/sample - loss: 0.0240 - accuracy: 0.9974 - val_loss: 0.4725 - val_accuracy: 0.8681\n",
      "Epoch 34/40\n",
      "15000/15000 [==============================] - 0s 14us/sample - loss: 0.0231 - accuracy: 0.9974 - val_loss: 0.4789 - val_accuracy: 0.8688\n",
      "Epoch 35/40\n",
      "15000/15000 [==============================] - 0s 14us/sample - loss: 0.0221 - accuracy: 0.9976 - val_loss: 0.4847 - val_accuracy: 0.8695\n",
      "Epoch 36/40\n",
      "15000/15000 [==============================] - 0s 14us/sample - loss: 0.0213 - accuracy: 0.9980 - val_loss: 0.4897 - val_accuracy: 0.8683\n",
      "Epoch 37/40\n",
      "15000/15000 [==============================] - 0s 13us/sample - loss: 0.0208 - accuracy: 0.9977 - val_loss: 0.4982 - val_accuracy: 0.8682\n",
      "Epoch 38/40\n",
      "15000/15000 [==============================] - 0s 14us/sample - loss: 0.0199 - accuracy: 0.9982 - val_loss: 0.5003 - val_accuracy: 0.8679\n",
      "Epoch 39/40\n",
      "15000/15000 [==============================] - 0s 14us/sample - loss: 0.0189 - accuracy: 0.9985 - val_loss: 0.5076 - val_accuracy: 0.8678\n",
      "Epoch 40/40\n",
      "15000/15000 [==============================] - 0s 14us/sample - loss: 0.0182 - accuracy: 0.9987 - val_loss: 0.5123 - val_accuracy: 0.8664\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                   partial_y_train,\n",
    "                   epochs=40,\n",
    "                   batch_size=512,\n",
    "                   validation_data=(x_val,y_val),\n",
    "                   verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 - 0s - loss: 0.5478 - accuracy: 0.8546\n",
      "[0.5477668739080429, 0.8546]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_data,  test_labels, verbose=2)\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
